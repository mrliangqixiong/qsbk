1.response是一个'scrapy.http.response.html.HtmlResponse'对象,可以执行'xpath'
  和'css'语法来提取数据.

2.提取出来的数据,是一个'selecor'或者是一个'selectorlist'对象,如果想要
  获取其中的字符串,那么应该执行'getall',或者'get'方法.

3.getall 方法: 获取的是'selector'中的所有文本.返回的是一个列表.

4.get方法: 获取的是'selector'中的第一个文本,返回的是一个str类型

5.如果数据解析回来,要传给pipeline处理,那么可以使用'yield'来返回,或者是收集所有的item,最后统一使用return返回.

6.item :建议在item.py中定义好模型,以后就不用使用字典.

7.pipeline : 这个是专门用来保存数据的,其中有三个方法是会经常用的:
 *'open_spider(self,spider)' : 当爬虫打开的时候执行.
 * 'process_item(self,item,spider)' : 当爬虫有item传过来的时候会被调用
 * 'close_spider(self,spider)' : 当爬虫关闭的时候会被调用.
 要激活pipeline,应该在'settings.py'中,设置'ITEM_PIPELINES'.
     ITEM_PIPELINES = {
       'qsbk.pipelines.QsbkPipeline': 300,
    }

## JsonItemExporter 和 JsonLinesItemExporter:
保存json数据的时候,可以使用这两个类,这操作变得更简单.
1.JsonItemExporter : 这个是每次把数据添加到内存中,最后统一写磁盘中,好处是存储的数据是一个满足json规则的数据,
    坏处是如果数据量比较大,那么比较耗内存.
2. JsonLinesItemExporter : 这个是每次调用'export_item'的时候就把这个item存储到硬盘中,坏处是每个字典是一行,
整个文件不是一个满足json格式的文件.好处是每次处理数据的时候就是直接存储到硬盘中,这样不会耗内存,数据也比较安全.

















